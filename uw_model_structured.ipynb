{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jt -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymssql\n",
    "import pandas as pd\n",
    "import json\n",
    "import multiprocessing as mp\n",
    "import datetime\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = '192.168.4.117'\n",
    "database = 'FreedomCashLenders'\n",
    "username = 'FreedomCashLendersAll'\n",
    "mssql_password = 'Freedom123$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iloans_conn = pymssql.connect(server, username, mssql_password, database, port = 1433)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"'2018-01-01'\"\n",
    "end_date = \"'2019-12-31'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_loan = f'''select LN.LoanId,\n",
    "                       LC.LoanCount,\n",
    "                       LN.OriginationDate,\n",
    "                       GC.BankReportData,\n",
    "                       GC.TimeAdded as ReportTimeAdded,\n",
    "                       LN.Campaign,\n",
    "                       LN.MonthlyGrossIncome,\n",
    "                       LN.DateOfBirth,\n",
    "                       LN.IsFirstDefault\n",
    "                       \n",
    "                from view_FCL_Loan LN\n",
    "                LEFT JOIN view_FCL_CustomerLoanCount LC ON LC.CustomerId = LN.CustomerId\n",
    "                LEFT JOIN view_FCL_GetCreditDataLoan GCD ON LN.LoanId = GCD.LoanId\n",
    "                LEFT JOIN view_FCL_GetCreditData GC ON GC.BankTransactionId = GCD.BankTransactionId\n",
    "                \n",
    "                \n",
    "                where LN.OriginationDate >= {start_date}\n",
    "                and LN.OriginationDate <= {end_date} \n",
    "                and LN.IsFirstDefault IS NOT NULL\n",
    "                and LN.MerchantId IN (15, 18)\n",
    "                and GC.ReportStatus = 'COMPLETE' '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loans = pd.read_sql_query(query_loan,con = iloans_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loans = df_loans.drop_duplicates('LoanId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_esign = f'''\n",
    "SELECT\n",
    "    LN.LoanId,\n",
    "    ESIG.AccessCount,\n",
    "    ESIG.EsigTimeSignedDiff_In_SEC\n",
    "FROM\n",
    "    view_FCL_Loan LN\n",
    "    LEFT JOIN view_FCL_EsignatureCustomerData ESIG ON LN.LoanId = ESIG.LoanId\n",
    "WHERE\n",
    "    LN.OriginationDate >= {start_date} \n",
    "    and LN.OriginationDate <= {end_date}\n",
    "    and LN.IsFirstDefault IS NOT NULL\n",
    "    and LN.MerchantId IN (15, 18)\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_esign = pd.read_sql_query(query_esign,con=iloans_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Data for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringify_account_ids(loan_id_list):\n",
    "    \"\"\"\n",
    "    Convert account_id list into comma-separated string of account_ids\n",
    "    :return: string containing comma-separated account_ids\n",
    "    \"\"\"\n",
    "    return '(' + ', '.join([str(i) for i in loan_id_list]) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loanid_string=stringify_account_ids(loan_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_loan_eval = '''select LN.LoanId,\n",
    "                       LC.LoanCount,\n",
    "                       LN.OriginationDate,\n",
    "                       GC.BankReportData,\n",
    "                       GC.TimeAdded as ReportTimeAdded,\n",
    "                       LN.Campaign,\n",
    "                       LN.MonthlyGrossIncome,\n",
    "                       LN.DateOfBirth,\n",
    "                       LN.IsFirstDefault\n",
    "                       \n",
    "                from view_FCL_Loan LN\n",
    "                LEFT JOIN view_FCL_CustomerLoanCount LC ON LC.CustomerId = LN.CustomerId\n",
    "                LEFT JOIN view_FCL_GetCreditDataLoan GCD ON LN.LoanId = GCD.LoanId\n",
    "                LEFT JOIN view_FCL_GetCreditData GC ON GC.BankTransactionId = GCD.BankTransactionId\n",
    "                \n",
    "                \n",
    "                WHERE\n",
    "                    GC.ReportStatus = 'COMPLETE'\n",
    "                    AND LN.LoanId IN %s'''%(loanid_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loan_eval = pd.read_sql_query(query_loan_eval,con=iloans_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loan_eval=df_loan_eval.drop_duplicates('LoanId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_esign_eval = '''\n",
    "SELECT\n",
    "    LN.LoanId,\n",
    "    ESIG.AccessCount,\n",
    "    ESIG.EsigTimeSignedDiff_In_SEC\n",
    "FROM\n",
    "    view_FCL_Loan LN\n",
    "    LEFT JOIN view_FCL_EsignatureCustomerData ESIG ON LN.LoanId = ESIG.LoanId\n",
    "WHERE\n",
    "    LN.LoanId IN %s'''%(loanid_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_esign_eval = pd.read_sql_query(query_esign_eval,con=iloans_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_esign_eval=df_esign_eval.drop_duplicates('LoanId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA and ML Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the importance of the new feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the following methodology to check whether a new engineered feature is meaningful or not:<br>\n",
    "1> Calculate the correlation of aal the features with the target(spearman correlation).<br>\n",
    "2> Check for the significance of that correlation.<br>\n",
    "3> Check if the feature causes multicolinearity in the dataset.<br>\n",
    "4> Check the Predictive Power score between all the features and the target.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(dataframe, new_feature, target, threshold = 0.9, return_corr_matrix = True, return_pps_matrix = True):\n",
    "    \"\"\"\n",
    "    Checks how important new_feature is w.r.t. the target on the basis of the PPS and spearman correlation.\n",
    "    Uses the PPS module to calculate Predictive Power Score.\n",
    "    ----------\n",
    "    Parameters:\n",
    "       dataframe(pandas df) : The dataframe consisting the whole dataset along with the new feature\n",
    "       feature(string) : Name of the new feature, as in the dataframe\n",
    "       target(string) : Name of the target, as in the dataframe\n",
    "       threshold(float) : default = 0.9; Multicolinearity threshold.\n",
    "       return_corr_matrix : default = True; Return the pps for the entire feature-space(not including the target).\n",
    "       return_pps_matrix : default = True; Return the pps for the entire feature-space(not including the target).\n",
    "    ----------   \n",
    "    Returns:\n",
    "       seaborn heatmap : **spearman correlation and pps score** b/w all possible features including the new one in a heatmap form.\n",
    "       float : **PPS score** of the new_feature with the target.\n",
    "       sloat : **Correlation** of new_feature with target if its numeric.\n",
    "       (float, float) : the **correlation and p-value** after the hypothesis testing(t-test for corr b/w new_feature and target).\n",
    "       list of tuples : tuples of all the features combinations which have higher pps, than the threshold, with the new feature.\n",
    "    \"\"\"\n",
    "    \n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    import ppscore as pps\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    from scipy.stats import pointbiserialr, spearmanr, chi2_contingency\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    df = dataframe\n",
    "    \n",
    "    # pps score\n",
    "    pps_score = pps.matrix(df)\n",
    "    pps_feat_tar = {}\n",
    "    for feature in df.drop([target], axis = 1).columns:\n",
    "        pps_feat_tar[feature] = pps_score.loc[target, feature]\n",
    "    \n",
    "    \n",
    "    # spearman correlation and pps score b/w all the features\n",
    "    if return_corr_matrix:\n",
    "        spearman_feat_2_feat = df.drop([target], axis = 1).corr(method = 'spearman')\n",
    "        pps_feat_2_feat = pps.matrix(df.drop([target], axis = 1))\n",
    "    \n",
    "    # pps score b/w all the features\n",
    "    if return_pps_matrix:\n",
    "        pps_feat_2_feat = pps.matrix(df.drop([target], axis = 1))\n",
    "    \n",
    "    # correlation testing\n",
    "    r = {}\n",
    "    for new_feature in df.drop([target], axis = 1).columns:\n",
    "        if df[new_feature].dtype == 'bool' or df[new_feature].dtype == 'O':\n",
    "            r[new_feature] = (chi2_contingency(pd.crosstab(df[new_feature], df[target]))[0], chi2_contingency(pd.crosstab(df[new_feature], df[target]))[1])\n",
    "        elif df[new_feature].dtype == 'int64' or df[new_feature].dtype == 'float64':\n",
    "            r[new_feature] = (pointbiserialr(df[new_feature], df[target])[0], pointbiserialr(df[new_feature], df[target])[1])\n",
    "        else:\n",
    "            print(\"check you new feature data type---should be one among [int64, bool, object]\")\n",
    "    \n",
    "    corr_tuplist = []\n",
    "    corr = pps_feat_2_feat\n",
    "    cols = len(corr.columns)\n",
    "    for i in range(corr.shape[0]):\n",
    "        for j in range(corr.shape[0]):\n",
    "            if (abs(corr.iloc[i, j]) > 0.9) and (i != j):\n",
    "                corr_tuplist.append((corr.index[i], corr.columns[j]))\n",
    "    \n",
    "    plt.figure(figsize = (16, 8))\n",
    "    print(\"Correlation heatmap\")\n",
    "    sns.heatmap(spearman_feat_2_feat, robust = 1, linewidth = 2, annot = True)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize = (16, 8))\n",
    "    print(\"PPS Score heatmap\")\n",
    "    sns.heatmap(pps_feat_2_feat, robust = 1, linewidth = 2, annot = True)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"PPS score details\\n\", pps_feat_tar)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"statistics-value & p-value of features wrt target(if feature was of boolean or object type, chi-square value shown) : \", r)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    if len(corr_tuplist) != 0:\n",
    "        print(\"Scores of feature_pairs exceeding the pps threshold : \", corr_tuplist)\n",
    "    else:\n",
    "        print(\"No feature pairs exceed the pps threshold\")\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Importing Libraries¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#ignore this code\n",
    "#this is just for increasing rhe width of the jupter cell\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # for reading the data into dataframes \n",
    "from pandas_profiling import ProfileReport # for in-depth eda\n",
    "import numpy as np # for faster numeric based calculations\n",
    "from pycaret.classification import * # for machine learning modelling\n",
    "from matplotlib import pyplot as plt # for plotting\n",
    "import seaborn as sns # for a high level plotting\n",
    "from sklearn.impute import KNNImputer as knn # for imputing missing or erraneous values in the dataset\n",
    "import cufflinks as cf #for annotating the plots\n",
    "from sklearn.ensemble import RandomForestClassifier # a random forest classifier\n",
    "from sklearn.tree import DecisionTreeClassifier # a decision tree based classifier\n",
    "from sklearn.model_selection import GridSearchCV# for hyperparameter tuning\n",
    "from sklearn.metrics import confusion_matrix as cm # for visualizing the predicted results\n",
    "from sklearn.naive_bayes import GaussianNB # for probailistic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up some styles for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style = \"darkgrid\", font = 'fantasy') # for seaborn based plotting\n",
    "cf.set_config_file(theme = 'ggplot', sharing = 'public', offline = True) # to show the graphs inside the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the training as well as testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('Downloads/training_data_2020-04-16.csv', header = 0, index_col = None)\n",
    "df_test = pd.read_csv('Downloads/eval_19-04-2020.csv', header = 0, index_col = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging both the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_train, df_test], ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets see the properties of training as well as testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape\n",
    "# training data consists of 14216 samples and 17 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape\n",
    "# testing data consists of 941 samples and 17 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping LoanID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As we can see that LoanID has a very large size and is of no use for prediction as well<br>\n",
    " So lets drop this feature before moving ahead from the joined dataset<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(['LoanId'], axis = 1)# deopping the loanID feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting boolean data types to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as boolean data types ane not well supported by the modules we're gonna use, its better to shift to string for both the joint as well as test dataset\n",
    "for i in range(df_train.shape[0]):\n",
    "    df_train.iloc[i, 1] = str(df_train.iloc[i, 1])\n",
    "    df_train.iloc[i, 4] = str(df_train.iloc[i, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the famous pycaret module for preprocessing, training, evaluation and predictions.<br>\n",
    "We will be following the mentioned workflow:<br>\n",
    "1. Preprocessing<br>\n",
    "2. Model training<br>\n",
    "3. Model hyperparameter tuning<br>\n",
    "4. Blending the top performing models<br>\n",
    "5. Plot-Evaluation<br>\n",
    "6. Prediction<br>\n",
    "7. Metric-Evaluation<br>\n",
    "8. Saving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing is an important step in the data mining process. The phrase \"garbage in, garbage out\" is particularly applicable to data mining and machine learning projects. Data-gathering methods are often loosely controlled, resulting in out-of-range values (e.g., Income: −100), impossible data combinations (e.g., Sex: Male, Pregnant: Yes), missing values, etc.<br>\n",
    "\n",
    "Analyzing data that has not been carefully screened for such problems can produce misleading results. Thus, the representation and quality of data is first and foremost before running an analysis. Often, data preprocessing is the most important phase of a machine learning project, especially in computational biology.<br>\n",
    "\n",
    "If there is much irrelevant and redundant information present or noisy and unreliable data, then knowledge discovery during the training phase is more difficult. Data preparation and filtering steps can take considerable amount of processing time. Data preprocessing includes cleaning, Instance selection, normalization, transformation, feature extraction and selection, etc. The product of data preprocessing is the final training set.<br>\n",
    "\n",
    "Data pre-processing may affect the way in which outcomes of the final data processing can be interpreted.<br>\n",
    "\n",
    "On that note, we can easily imagine how tedious and time consuming this step could be. Inorder to solve this dillema, we are gonna use the pycaret's setup function that does all the preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's summarize how the profile discussed in section **2.8** has helped make critical pre-processing choices with the data.<br>\n",
    "\n",
    "**Missing Values**: There are no missing values in the data. However, we still need imputers in our pipeline just in case the new unseen data has missing values (not applicable in this case). When you execute the setup() function, imputers are created and stored in the pipeline automatically. By default, it uses a mean imputer for numeric values and a constant imputer for categorical. This can be changed using the numeric_imputation and categorical_imputation parameters in setup().<br>\n",
    "\n",
    "**Multicollinearity**: There are high correlations between LenderAmountDeb30 and LenderAmountDeb as well as between LenderCountDeb and LenderCountDeb30, which introduces multicollinearity into the data. We will remove multi-collinearity by using the remove_multicollinearity and multicollinearity_threshold parameters in setup.<br>\n",
    "\n",
    "**Data Scale / Range**: Notice how the scale / range of numeric features are different. For example the Age feature ranges from between 25 to 87, EsigTimeSignedDiff_In_SEC ranges from -264 to 2,62,83,332 and MonthlyGrossIncome ranges from 400 to 1,00,000. This may cause problems for algorithms that assume all features have variance within the same order. In this case, the order of magnitude for all these variables is very different from each other. We will deal with this problem by using the normalize parameter in setup.<br>\n",
    "\n",
    "**Distribution of Feature Space**: Numeric features are not normally distributed. Look at the distributions of Age, EsigTimeSignedDiff_In_SEC and MonthlyGrossIncome. A few features are also highly skewed such as AccessCount and EsigTimeSignedDiff_In_SEC. This may cause problems for algorithms that assume normal or approximate normal distributions of the data. Examples include Logistic Regression, Linear Discriminant Analysis (LDA) and Naive Bayes. We will deal with this problem by using the transformation parameter in setup.<br>\n",
    "\n",
    "**Group Features**: From the data description we know that certain features are related with each other such as LenderAmountCred and LenderAmountCred30 and more features which are related from a sample level. We will use the group_features parameter in setup to extract statistical information from these features.<br>\n",
    "\n",
    "**Bin Numeric Features**: When looking at the correlations between the numeric features and the target variable, we that Age is weak. We will use the bin_numeric_features parameter to remove the noise from these variables which may help linear algorithms.<br>\n",
    "\n",
    "**Combine Rare Levels**: Sometimes a dataset can have a categorical feature (or multiple categorical features) that has a very high number of levels (i.e. high cardinality features). If such feature (or features) are encoded into numeric values, then the resultant matrix is a sparse matrix. This not only makes experiment slow due to manifold increment in the number of features and hence the size of the dataset, but also introduces noise in the experiment. Sparse matrix can be avoided by combining the rare levels in the feature(or features) having high cardinality.<br>\n",
    "\n",
    "**Ignore Low Variance**: Sometimes a dataset may have a categorical feature with multiple levels, where distribution of such levels are skewed and one level may dominate over other levels. This means there is not much variation in the information provided by such feature.  For a ML model, such feature may not add a lot of information and thus can be ignored for modeling.Both conditions below must be met for a feature to be considered a low variance feature:<br>\n",
    "    Count of unique values in a feature  / sample size < 10%<br>\n",
    "    Count of most common value / Count of second most common value > 20 times.<br>\n",
    "\n",
    "**Feature Interaction**: It is often seen in machine learning experiments when two features combined through an arithmetic operation becomes more significant in explaining variances in the data, than the same two features separately. Creating a new feature through interaction of existing features is known as feature interaction. It can achieved in PyCaret using feature_interaction and feature_ratio parameters within setup. Feature interaction creates new features by multiplying two variables (a * b), while feature ratios create new features but by calculating the ratios of existing features (a / b).<br>\n",
    "\n",
    "**Polynomial Features**: In machine learning experiments, the relationship between the dependent and independent variable is often assumed as linear, however this is not always the case. Sometimes the relationship between dependent and independent variables is more complex. Creating new polynomial features sometimes might help in capturing that relationship which otherwise may go unnoticed.<br>\n",
    "\n",
    "**Remove Outliers**: The Remove Outliers function in PyCaret allows you to identify and remove outliers from the dataset before training the model. Outliers are identified through PCA linear dimensionality reduction using the Singular Value Decomposition technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw previously, EsigTimeSignedDiff_In_SEC contains many negative as well as positive extreme value which need to be capped with something sensible. Hence, for the time being, we will be using KNN Imputer which imputes on the basis of nearest neighbors in the high dimensional feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_knn = knn(missing_values = np.nan, n_neighbors = 7)# initiating the imputer with parameters\n",
    "\n",
    "# imputing the extremeties which dont make any sense for this particukar feature\n",
    "for i in df_train[df_train['EsigTimeSignedDiff_In_SEC'] < 0 ].index:\n",
    "    df_train.loc[i, 'EsigTimeSignedDiff_In_SEC'] = np.nan\n",
    "    \n",
    "for i in df_train[df_train['EsigTimeSignedDiff_In_SEC'] > 432000 ].index:\n",
    "    df_train.loc[i, 'EsigTimeSignedDiff_In_SEC'] = np.nan\n",
    "\n",
    "# finally, imputing all the capped values \n",
    "df_train[['EsigTimeSignedDiff_In_SEC']] = imputer_knn.fit_transform(df_train[['EsigTimeSignedDiff_In_SEC']])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over-sampling the minority class to reduce the imbalance to a certain extent, using the Adaptive Synthetic (ADASYN) sampling approach for imbalanced datasets. You can read the documentation here : <br>https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.ADASYN.html#imblearn.over_sampling.ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.get_dummies(df_train, columns = ['Reloan', 'LeadProvider'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_new['IsFirstDefault']\n",
    "X = df_new.drop(['IsFirstDefault'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversmp = ADASYN(random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_res, y_res = oversmp.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = X_res\n",
    "df_new['IsFirstDefault'] = y_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to feed the dataset inside the pipeline, all the uint type datatypes need to be converted into int32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[['Reloan_False', 'Reloan_True', 'LeadProvider_EPCVIP',\n",
    "       'LeadProvider_Freedom', 'LeadProvider_ITMedia', 'LeadProvider_LeadPie',\n",
    "       'LeadProvider_LeadsMarket', 'LeadProvider_LeapThry',\n",
    "       'LeadProvider_Nimbus', 'LeadProvider_PingBid', 'LeadProvider_RoundSky',\n",
    "       'LeadProvider_Roundsky', 'LeadProvider_StopNGo', 'LeadProvider_Zero']] = df_new[['Reloan_False', 'Reloan_True', 'LeadProvider_EPCVIP',\n",
    "                                                                                        'LeadProvider_Freedom', 'LeadProvider_ITMedia', 'LeadProvider_LeadPie',\n",
    "                                                                                        'LeadProvider_LeadsMarket', 'LeadProvider_LeapThry',\n",
    "                                                                                        'LeadProvider_Nimbus', 'LeadProvider_PingBid', 'LeadProvider_RoundSky',\n",
    "                                                                                        'LeadProvider_Roundsky', 'LeadProvider_StopNGo', 'LeadProvider_Zero']].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The setup() function initializes the environment in pycaret and creates the transformation pipeline to prepare the data for modeling and deployment. setup() must be called before executing any other function in pycaret. It takes two mandatory parameters: a pandas dataframe and the name of the target column.<br>\n",
    "\n",
    "When setup() is executed, PyCaret's inference algorithm will automatically infer the data types for all features based on certain properties. The data type should be inferred correctly but this is not always the case. To account for this, PyCaret displays a table containing the features and their inferred data types after setup() is executed. If all of the data types are correctly identified enter can be pressed to continue or quit can be typed to end the expriment. Ensuring that the data types are correct is of fundamental importance in PyCaret as it automatically performs a few pre-processing tasks which are imperative to any machine learning experiment. These tasks are performed differently for each data type which means it is very important for them to be correctly configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = setup(data = df_new, train_size = .99, target = 'IsFirstDefault', session_id = 769,\n",
    "            normalize = True,\n",
    "            combine_rare_levels = True,\n",
    "            transformation = True,\n",
    "            ignore_low_variance = True, \n",
    "            remove_multicollinearity = True,\n",
    "            bin_numeric_features = ['Age'],\n",
    "            group_features = [['LenderAmountCred', 'LenderAmountCred30'], ['LenderCountCred', 'LenderCountCred30']],\n",
    "            feature_interaction = True, feature_ratio = True,\n",
    "            feature_selection = True,\n",
    "            polynomial_features = True,\n",
    "            remove_outliers = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the preprocessed data from the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = clf[0]\n",
    "y_train = clf[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = clf[3]\n",
    "y_test = clf[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a model is created using the create_model() function it uses the default hyperparameters. In order to tune hyperparameters, the tune_model() function is used. This function automatically tunes the hyperparameters of a model on a pre-defined search space and scores it using stratified cross validation. The output prints a score grid that shows Accuracy, Recall, Precision, F1 and Kappa by fold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_rf = tune_model('rf', optimize = 'F1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_lgbm = tune_model('lightgbm', optimize = 'F1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_nb = tune_model('nb', optimize = 'F1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagged_nb = ensemble_model(tuned_nb, method = 'Bagging')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blending the top performing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blending is another common technique for ensembling that can be used in PyCaret. It uses predictions from multiple models to generate a final set of predictions using voting / majority consensus from all of the models passed in the estimator_list parameter. If no list is passed, PyCaret uses all of the models available in the model library by default. The method parameter can be used to define the type of voting. When set to hard, it uses labels for majority rule voting. When set to soft it uses the sum of predicted probabilities instead of the label. we will stick to the soft method. We will be passing our top 2 handpicked models for blending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blend_specific_soft = blend_models(estimator_list = [tuned_rf, bagged_nb, tuned_lgbm], method = 'soft')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our final model is ready for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot-Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at some plots to see how our model is performing on the test data, which was set as the 5% data from the training dataset passed in the setup function in section 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(blend_specific_soft, plot = 'confusion_matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(blend_specific_soft, plot = 'auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision-Recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(blend_specific_soft, plot = 'pr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output dataframe is the test data separated earlier by the setup function which has additional 2 columns, viz the predicted Label and the confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets finalize the model and see how it performs on our complete as well as personal holdout set viz df and df_test respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = finalize_model(blend_specific_soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on complete dataset\n",
    "predict_model(final_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get the test data in the same format as well in-order to get a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_new = pd.get_dummies(df_test, columns = ['Reloan', 'LeadProvider'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on the test or holdout set\n",
    "predictions = predict_model(blend_specific_soft, data = df_test_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric-Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the KS metric score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to modify the predictions dataframe in order to calculate the KS score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(predictions.shape[0]):\n",
    "    predictions.iloc[i, 2] = str(predictions.iloc[i, 2])\n",
    "    \n",
    "for i in range(predictions.shape[0]):\n",
    "    if predictions.iloc[i, 1] == 'True':\n",
    "        predictions.iloc[i, 1] = 1\n",
    "    else:\n",
    "        predictions.iloc[i, 1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The KS Score        \n",
    "df_scores = predictions.sort_values(by = 'Score')\n",
    "total_good = (predictions['IsFirstDefault'] == 0).sum()\n",
    "total_bad = (df_scores['IsFirstDefault'] == 1).sum()\n",
    "df_scores['cum_good_perc'] = (df_scores['IsFirstDefault'] == 0).cumsum()/total_good\n",
    "df_scores['cum_bad_perc'] = (df_scores['IsFirstDefault'] == 1).cumsum()/total_bad\n",
    "df_scores['cum_diff'] = np.abs((df_scores['cum_good_perc'] - df_scores['cum_bad_perc']))\n",
    "df_scores['cum_diff'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now save the model so that it could be loaded next time and could be used for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(final_model, 'final_blend-nb+knc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dates(json_date):\n",
    "    '''\n",
    "    Converts json formatted date to pandas datetime.\n",
    "    \n",
    "    Parameters:\n",
    "    JSON date (JSON).\n",
    "    \n",
    "    Returns:\n",
    "    Pandas datetime object.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #return datetime.fromtimestamp(int(json_date)/1000.0).strftime('%Y-%m-%d')\n",
    "    return datetime.datetime.utcfromtimestamp(int(json_date)/1000).date()\n",
    "\n",
    "\n",
    "def fetch_checking_acct_txns(json_string):\n",
    "    \"\"\"\n",
    "    Parse all checking account transactions in the bank report\n",
    "    \n",
    "    Parameters:\n",
    "    json_string(json): json containing bank report\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: containing transactions \n",
    "    \n",
    "    \"\"\"\n",
    "    j = json.loads(json_string)\n",
    "    df_txn = pd.DataFrame()\n",
    "    \n",
    "    acct_numbers = []\n",
    "    for accts in j['accounts']:\n",
    "        \n",
    "        if ('transactions' in accts.keys()) and (len(accts['transactions']) > 0) and (accts['accountNumber'] not in acct_numbers) and (accts['accountType'].strip().lower() == 'checking'):\n",
    "            \n",
    "            df_txn_temp = pd.DataFrame(accts['transactions'])\n",
    "            df_txn_temp['account_number'] = accts['accountNumber']\n",
    "            df_txn = df_txn.append(df_txn_temp, ignore_index=True)\n",
    "            \n",
    "            df_txn['posted_date'] = df_txn['postedDate'].map(lambda json_date: parse_dates(json_date))\n",
    "            df_txn['category'] = df_txn['contexts'].map(lambda x: x[0]['categoryName'] if len(x) > 0 else np.nan)\n",
    "            acct_numbers.append(accts['accountNumber'])\n",
    "    \n",
    "    if 'pending' in df_txn.columns:\n",
    "        df_txn = df_txn[df_txn['pending'] == False]\n",
    "    return df_txn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## primary account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_primary_account(bankreport):\n",
    "    \"\"\"\n",
    "    Flag primary checking account (account having max transaction count)\n",
    "    \n",
    "    Parameters:\n",
    "    bankreport (json)\n",
    "    loanid (str)\n",
    "    \n",
    "    Returns:\n",
    "    Dataframe containing checking accounts and primary account flag = 1\n",
    "    \"\"\"\n",
    "    df_txn = fetch_checking_acct_txns(bankreport)\n",
    "    if df_txn.empty is False:\n",
    "        df_txns_count = df_txn['account_number'].value_counts()\n",
    "        return df_txns_count.idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NCPU = mp.cpu_count() - 2 if mp.cpu_count() > 2 else 1\n",
    "with mp.Pool(processes=NCPU) as pool:\n",
    "    res_primary_accts = pool.map(get_primary_account, df_loans['BankReportData'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loans['primary_account'] = res_primary_accts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loans = df_loans.loc[df_loans['primary_account'].notnull(),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter loans having transaction days >= 60 in primary account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transaction_days_count(primary_account,bank_report):\n",
    "    df_checking_txns = fetch_checking_acct_txns(bank_report)\n",
    "    if df_checking_txns.empty is False:\n",
    "        df_primary_account_txns = df_checking_txns[df_checking_txns['account_number']==primary_account]\n",
    "        df_primary_account_txns= df_primary_account_txns.sort_values(by='posted_date')\n",
    "        first_txn_date = df_primary_account_txns['posted_date'].iloc[0]\n",
    "        last_txn_date = df_primary_account_txns['posted_date'].iloc[-1]\n",
    "        txn_days_count = (last_txn_date - first_txn_date).days\n",
    "        return txn_days_count >= 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NCPU = mp.cpu_count() - 2 if mp.cpu_count() > 2 else 1\n",
    "with mp.Pool(processes=NCPU) as pool:\n",
    "    txn_days_count = pool.starmap(get_transaction_days_count, zip(df_loans['primary_account'],df_loans['BankReportData']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loans['txn_days_count'] = txn_days_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loans = df_loans.loc[df_loans['txn_days_count'] == True, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_age(current_date, dob):\n",
    "    age = len(pd.date_range(start=dob,end=current_date,freq='Y'))\n",
    "    return age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loans['Age'] = df_loans.apply(lambda x: calculate_age(x['OriginationDate'],x['DateOfBirth']), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New or Reloan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loans['Reloan'] = df_loans['LoanCount'].apply(lambda x:True if x>1 else False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lead Provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loans = df_loans.loc[df_loans['Campaign'].notnull(),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_provider_list=['MarketBullet','StopNGo','Nimbus','EPCVIP','PingBid','LeapThry',\n",
    "'Acquir','RoundSky','Zero','LeadPie',\n",
    "'ITMedia','LeadsMarket']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loans['LeadProvider'] = df_loans['Campaign'].str.extract(\"(\" + \"|\".join(lead_provider_list) +\")\",flags = re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loans['LeadProvider']=df_loans['LeadProvider'].fillna('Freedom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lender vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lender_vars(loanid,report_string,time_added,pr_acct):\n",
    "\n",
    "        \"\"\"\n",
    "        Function to generate lender variables \n",
    "        from primary account transactions\n",
    "\n",
    "        Paramaters:\n",
    "            txns(Boolean): True - Return lender txns along with lender variables\n",
    "                           False - Return only lender variables\n",
    "\n",
    "        Returns:\n",
    "            lender_vars(dictionary): Dictionary containing all lender variables\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        lender_vars = dict()   \n",
    "        lender_vars['LoanId'] = loanid\n",
    "        lender_vars['LenderAmountDeb'] = 0.0\n",
    "        lender_vars['LenderCountCred'] = 0.0\n",
    "        lender_vars['LenderAmountCred30'] = 0.0\n",
    "        lender_vars['LenderCountDeb'] = 0.0\n",
    "        lender_vars['LenderAmountDeb30'] = 0.0\n",
    "        lender_vars['LenderCountCred30'] = 0.0\n",
    "        lender_vars['LenderCountDeb30'] = 0.0\n",
    "        lender_vars['LenderAmountCred'] = 0.0\n",
    "        lender_vars['UniqLenderCount'] = 0.0\n",
    "\n",
    "        #load lending company list\n",
    "        lend_cos=joblib.load('./lend_cos.pkl')\n",
    "\n",
    "        #get primary checking account transactions\n",
    "        df_checking_txns = fetch_checking_acct_txns(report_string) \n",
    "        df_pr_acct_txns = df_checking_txns[df_checking_txns['account_number']==pr_acct]\n",
    "        \n",
    "        \n",
    "        #prepare lender transactions dataframe\n",
    "        df_lender_txns=df_pr_acct_txns.loc[df_pr_acct_txns['memo'].str.contains('|'.join(lend_cos),case=False,na=False)]\n",
    "        \n",
    "        #check for empty transactions\n",
    "        if df_lender_txns.empty is False:\n",
    "            df_lender_txns['lenderName'] = df_lender_txns['memo'].str.extract(\"(\" + \"|\".join(lend_cos) +\")\",flags = re.IGNORECASE)\n",
    "            df_lender_txns['days_diff'] = (time_added.date()-df_lender_txns['posted_date']).dt.days\n",
    "            df_lender_txns['amount'] = df_lender_txns['amount'].round(2)\n",
    "\n",
    "\n",
    "            #conditions to determine lender variables\n",
    "            cond1 = (df_lender_txns['amount']>0)\n",
    "            cond2 = cond1 & (df_lender_txns['days_diff']<=30)\n",
    "            cond3 = (df_lender_txns['amount']<0)\n",
    "            cond4 = cond3 & (df_lender_txns['days_diff']<=30)\n",
    "\n",
    "            #prepare lender variables\n",
    "            lender_vars['LenderAmountDeb'] = float(df_lender_txns.loc[cond3,'amount'].sum())\n",
    "            lender_vars['LenderCountCred'] = float(df_lender_txns[cond1].shape[0])\n",
    "            lender_vars['LenderAmountCred30'] = float(df_lender_txns.loc[cond2,'amount'].sum())\n",
    "            lender_vars['LenderCountDeb'] = float(df_lender_txns[cond3].shape[0])\n",
    "            lender_vars['LenderAmountDeb30'] = float(df_lender_txns.loc[cond4,'amount'].sum())\n",
    "            lender_vars['LenderCountCred30'] = float(df_lender_txns.loc[cond2].shape[0])\n",
    "            lender_vars['LenderCountDeb30'] = float(df_lender_txns.loc[cond4].shape[0])\n",
    "            lender_vars['LenderAmountCred'] = float(df_lender_txns.loc[cond1,'amount'].sum())\n",
    "            lender_vars['UniqLenderCount'] = float(df_lender_txns['lenderName'].nunique())\n",
    "\n",
    "        return pd.DataFrame(lender_vars,index=[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### in case the lender vars are to be generated for funded loans between 2018-01-01 to 2019-12-31 do not run the below cell, instead download from s3 ( look for \"download lender vars from s3\" markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lender_vars = pd.DataFrame()\n",
    "NCPU = mp.cpu_count() - 2 if mp.cpu_count() > 2 else 1\n",
    "with mp.Pool(processes=NCPU) as pool:\n",
    "    df_lender_vars_temp = pool.starmap(create_lender_vars, zip(df_loans['LoanId'],df_loans['BankReportData'],df_loans['ReportTimeAdded'],df_loans['primary_account']))\n",
    "df_lender_vars=pd.concat(df_lender_vars_temp,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lender_vars.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### download lender vars from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provide access keys if needed\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.download_file('predicon-bucket', 'lender_vars.csv', 'FILE_NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lender_vars = pd.read_csv('lender_vars.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loans = pd.merge(df_loans,df_lender_vars,how='left',on='LoanId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### esign variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loans= pd.merge(df_loans,df_esign,on='LoanId',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## useful links\n",
    "https://docs.databricks.com/_static/notebooks/mlflow/mlflow-quick-start-deployment-aws.html\n",
    "\n",
    "https://towardsdatascience.com/deploying-models-to-production-with-mlflow-and-amazon-sagemaker-d21f67909198\n",
    "\n",
    "https://www.h2o.ai/blog/a-deep-dive-into-h2os-automl/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## specify features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_drop = ['LoanCount',\n",
    "'OriginationDate',             \n",
    "'BankReportData',                   \n",
    "'ReportTimeAdded',                  \n",
    "'Campaign',\n",
    "'primary_account',\n",
    "'txn_days_count', 'DateOfBirth',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_loans.drop(columns=features_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "h2o.init(max_mem_size='16G')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h20_train =  h2o.H2OFrame(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = \"IsFirstDefault\" \n",
    "x = df_h20_train.columns\n",
    "x.remove(y)\n",
    "x.remove('LoanId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml = H2OAutoML(max_runtime_secs=120, seed=1)\n",
    "aml.train(x=x, y=y, training_frame=df_h20_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = aml.leaderboard\n",
    "lb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### primary account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NCPU = mp.cpu_count() - 2 if mp.cpu_count() > 2 else 1\n",
    "with mp.Pool(processes=NCPU) as pool:\n",
    "    res_primary_accts = pool.map(get_primary_account, df_loan_eval['BankReportData'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loan_eval['primary_account'] = res_primary_accts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loan_eval = df_loan_eval.loc[df_loan_eval['primary_account'].notnull(),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter loans having transaction days >= 60 in primary account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NCPU = mp.cpu_count() - 2 if mp.cpu_count() > 2 else 1\n",
    "with mp.Pool(processes=NCPU) as pool:\n",
    "    txn_days_count = pool.starmap(get_transaction_days_count, zip(df_loan_eval['primary_account'],df_loan_eval['BankReportData']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loan_eval['txn_days_count'] = txn_days_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loan_eval = df_loan_eval.loc[df_loan_eval['txn_days_count']==True,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loan_eval['Age'] = df_loan_eval.apply(lambda x: calculate_age(x['OriginationDate'],x['DateOfBirth']), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lead provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loan_eval = df_loan_eval.loc[df_loan_eval['Campaign'].notnull(),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loan_eval['LeadProvider'] = df_loan_eval['Campaign'].str.extract(\"(\" + \"|\".join(lead_provider_list) +\")\",flags = re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loan_eval['LeadProvider']=df_loan_eval['LeadProvider'].fillna('Freedom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### new or reloan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loan_eval['Reloan'] = df_loan_eval['LoanCount'].apply(lambda x:True if x>1 else False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lender vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lender_vars_eval = pd.DataFrame()\n",
    "NCPU = mp.cpu_count() - 2 if mp.cpu_count() > 2 else 1\n",
    "with mp.Pool(processes=NCPU) as pool:\n",
    "    df_lender_vars_temp = pool.starmap(create_lender_vars, zip(df_loan_eval['LoanId'],df_loan_eval['BankReportData'],df_loan_eval['ReportTimeAdded'],df_loan_eval['primary_account']))\n",
    "df_lender_vars_eval=pd.concat(df_lender_vars_temp,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loan_eval = pd.merge(df_loan_eval,df_lender_vars_eval,on='LoanId',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### esign variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loan_eval= pd.merge(df_loan_eval,df_esign_eval,on='LoanId',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loan_predict = df_loan_eval.drop(columns=features_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o_eval = h2o.H2OFrame(df_loan_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = aml.leader.predict(h2o_eval)\n",
    "pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to pandas dataframe\n",
    "df_predictions = h2o.as_list(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loan_eval.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions['target'] = df_loan_eval['IsFirstDefault']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = df_predictions.rename(columns={'True':'prob'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = df_predictions[['target','prob']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions['target'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get BV uncertain and BV Approved loans for model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username_bank_app = 'bankreview'\n",
    "password_bank_app = 'Freedom!23'\n",
    "host_bank_app = '192.168.4.115'\n",
    "port_bank_app = 3306\n",
    "db_bank_app = 'bankreviewdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_app_conn = pymysql.connect(host=host_bank_app,\n",
    "                                port=port_bank_app,\n",
    "                                db=db_bank_app,\n",
    "                                user=username_bank_app,\n",
    "                                password=password_bank_app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_evaluation_loans = '''select loan_id, \n",
    "                                final_decision,\n",
    "                                reasons_for_decision,\n",
    "                                entered_date\n",
    "                                \n",
    "                            from loan \n",
    "                            where campaign like '%Production%'\n",
    "                            and STR_TO_DATE(entered_date ,'%m/%d/%Y') >= STR_TO_DATE('01/01/2020','%m/%d/%Y')\n",
    "                            and STR_TO_DATE(entered_date ,'%m/%d/%Y') < STR_TO_DATE('04/01/2020','%m/%d/%Y')\n",
    "                            and final_decision in ('Bank Validation Uncertain','Bank Validation Approved') '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_loans = pd.read_sql_query(query_evaluation_loans, con = bank_app_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get funded and mature loans for the same period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_funded_mature_loans = ''' select LoanId, \n",
    "                                IsFirstDefault\n",
    "                        from view_FCL_Loan\n",
    "                        where OriginationDate >= '2020-01-01' \n",
    "                        and OriginationDate <= '2020-03-31'\n",
    "                        and IsFirstDefault IS NOT NULL\n",
    "                        and MerchantId IN (15, 18)\n",
    "                        \n",
    "                     '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_funded_mature_loans = pd.read_sql_query(query_funded_mature_loans,con = iloans_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_funded_mature_loans['LoanId'] = df_funded_mature_loans['LoanId'].astype(int).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.merge(df_funded_mature_loans,df_eval_loans,how = 'inner',left_on = 'LoanId',right_on = 'loan_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_id_list = list(df_eval['LoanId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute KS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_KS(df_pred):\n",
    "    \"\"\"\n",
    "    Returns KS given scores\n",
    "    Parameters:\n",
    "    df_pred (pandas df): DataFrame containing target variable and model score\n",
    "    \n",
    "    Returns:\n",
    "    float: KS value\n",
    "    \"\"\"\n",
    "    df_scores = df_pred.sort_values(by='prob')\n",
    "    total_good = (df_scores['target'] == False).sum()\n",
    "    total_bad = (df_scores['target'] == True).sum()\n",
    "    df_scores['cum_good_perc'] = (df_scores['target'] == False).cumsum()/total_good\n",
    "    df_scores['cum_bad_perc'] = (df_scores['target'] == True).cumsum()/total_bad\n",
    "    df_scores['cum_diff'] = np.abs((df_scores['cum_good_perc'] - df_scores['cum_bad_perc']))\n",
    "    return df_scores['cum_diff'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_KS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quantiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_table(df_pred,n = 10):\n",
    "    \"\"\"\n",
    "    Returns a quantile table given model scores (default is decile)\n",
    "    \n",
    "    Parameters:\n",
    "    df_pred (pandas df): DataFrame containing target variable and model score\n",
    "    \n",
    "    Returns:\n",
    "    pandas DataFrame: Pandas dataframe containing quantiles\n",
    "    \n",
    "    \"\"\"\n",
    "    df_scores = df_pred.sort_values(by='prob')\n",
    "    df_scores['decile'],score_bin = pd.qcut(df_scores['prob'],10,labels=[1,2,3,4,5,6,7,8,9,10],retbins = True)\n",
    "    df_scores['target'] = df_scores['target'].astype(int)\n",
    "    df_scores_deciles = df_scores.groupby('decile',as_index=False).agg({'prob':['count','min','max','mean'],'target':'sum'})\n",
    "    df_scores_deciles.columns = ['decile','count','min_score','max_score','mean_score','bad_count']\n",
    "    df_scores_deciles['perc_bad'] = (df_scores_deciles['bad_count']/df_scores_deciles['count']) * 100\n",
    "    return df_scores_deciles,score_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_table, score_bins = quantile_table(df_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get bins for quantile assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_bins = np.concatenate(([-np.inf], score_bins, [np.inf]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upload to sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.h2o as mh2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.sagemaker as mfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mh2o.save_model(aml.leader,path=\"path/to/trained/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"us-east-1\"\n",
    "arn = \"arn:aws:iam::757719720041:role/Sagemaker\"\n",
    "appname = \"h20-mlflow-deploy\"\n",
    "modeluri = \"path/to/saved/model\" \n",
    "image_url = \"757719720041.dkr.ecr.us-east-1.amazonaws.com/freedom-pyfunc:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfs.deploy(app_name=appname, model_path=modeluri, instance_type='ml.t2.medium',region_name=region, mode=\"create\",execution_role_arn=arn,image_url=image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def check_status(app_name):\n",
    "    sage_client = boto3.client('sagemaker', region_name=\"us-east-1\")\n",
    "    endpoint_description = sage_client.describe_endpoint(EndpointName=app_name)\n",
    "    endpoint_status = endpoint_description[\"EndpointStatus\"]\n",
    "    return endpoint_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_status(appname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
