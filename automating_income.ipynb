{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import multiprocessing as mp\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding some paths to use from which we would be fetching useful modules like '/home/shared/utils' has db_utils module which is used to connect to the server without showing the credentials!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.getcwd())\n",
    "sys.path.insert(0, '/home/shared/utils')\n",
    "sys.path.insert(0, '/home/vishal/refactoring_pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function has fetch_checking_acct_txns which is used to check which account from all the available accounts of candidate is checking account. Similarly, EDA is used to get plots which are useful to analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query function is now available where we dont even have to add credentials in the python dile to establish connection It uses a yaml config file to establish connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db_utils import *\n",
    "from helper import fetch_checking_acct_txns\n",
    "import EDA as eda\n",
    "import query as q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NCPU is for multiprocessing. The function below takes all the CPU cores available in the system except 2 and if the system only has 2 cores, it uses only one of the cores to perform operations!!(FUn fact:The server which we are working on has 16 spu cores!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NCPU = mp.cpu_count() - 2 if mp.cpu_count() > 2 else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query to fetch required data from iloans!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_required_bank_reports(start, end):\n",
    "    query = f'''\n",
    "                SELECT\n",
    "                    LN.LoanId,\n",
    "                    GCD.TimeAdded,\n",
    "                    LN.OriginationDate,\n",
    "                    LN.FirstName,\n",
    "                    LN.LastName,\n",
    "                    LN.Campaign,\n",
    "                    LN.OriginalPrincipal,\n",
    "                    LN.ReUppedPrincipal,\n",
    "                    LN.DateOfBirth,\n",
    "                    LN.BankName,\n",
    "                    LN.MonthlyGrossIncome,\n",
    "                    LN.EmployerName,\n",
    "                    LN.IsFirstDefault,\n",
    "                    GCD.BankTransactionId,\n",
    "                    GCD.BankReportData\n",
    "                FROM view_FCL_Loan LN\n",
    "                LEFT JOIN view_FCL_GetCreditDataLoan GCDL on LN.LoanId = GCDL.LoanId\n",
    "                LEFT JOIN view_FCL_GetCreditData GCD on GCD.BankTransactionId = GCDL.BankTransactionId\n",
    "                WHERE LN.OriginationDate >= {start}\n",
    "                AND LN.OriginationDate <= {end}\n",
    "                AND LN.IsFirstDefault IS NOT NULL\n",
    "                AND LN.MerchantId IN (15, 18)\n",
    "                AND GCD.ReportStatus  = 'COMPLETE' \n",
    "            '''\n",
    "    df = q.iloans(query)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query to fetch the json data which has all the income related information from bankapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_required_bank_app(start, end):\n",
    "    query = f'''\n",
    "                SELECT\n",
    "                    loan_id as LoanId,\n",
    "                    json\n",
    "                FROM loan\n",
    "                WHERE campaign like '%Production%'\n",
    "                AND STR_TO_DATE(entered_date, '%m/%d/%Y') >= STR_TO_DATE({start}, '%Y-%m-%d')\n",
    "                AND STR_TO_DATE(entered_date, '%m/%d/%Y') <= STR_TO_DATE({end}, '%Y-%m-%d')\n",
    "             '''\n",
    "    df = q.bankapp(query)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching the primary checking account coz a candidate may have multiple checking accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_primary_account(bankreport):\n",
    "    \"\"\"\n",
    "    Flag primary checking account (account having max transaction count)\n",
    "    \n",
    "    Args:\n",
    "    bankreport (json)\n",
    "    loanid (str)\n",
    "    \n",
    "    Returns:\n",
    "    account number (str) : account number of primary account\n",
    "    \"\"\"\n",
    "    df_txn = fetch_checking_acct_txns(bankreport)\n",
    "    if df_txn.empty is False:\n",
    "        df_txns_count = df_txn['account_number'].value_counts()\n",
    "        return df_txns_count.idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We would be working on only those applicants who have atleast 60 days of transactions, from the applied date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transaction_days_count(primary_account, bank_report):\n",
    "    \"\"\"Checks if number of transaction days >=60 given an account\n",
    "    \n",
    "    Args:\n",
    "    primary_account (str): Account number of primary account\n",
    "    bank_report (str): bank report string\n",
    "\n",
    "    Returns:\n",
    "    True or False (bool)\n",
    "    \"\"\" \n",
    "    df_checking_txns = fetch_checking_acct_txns(bank_report)\n",
    "    if df_checking_txns.empty is False:\n",
    "        df_primary_account_txns = df_checking_txns[df_checking_txns['account_number'] == primary_account]\n",
    "        df_primary_account_txns= df_primary_account_txns.sort_values(by = 'posted_date')\n",
    "        first_txn_date = df_primary_account_txns['posted_date'].iloc[0]\n",
    "        last_txn_date = df_primary_account_txns['posted_date'].iloc[-1]\n",
    "        txn_days_count = (last_txn_date - first_txn_date).days\n",
    "        return txn_days_count >= 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The function below returns all the transactions for a candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transaction_time_series(primary_account, bank_report, loan_id):\n",
    "    \"\"\"Compute transactions of each day with the dates in timeseries format.\n",
    "\n",
    "    Args:\n",
    "    loan_id (float)\n",
    "    bank_report (str)\n",
    "    primary_acct (str)\n",
    "\n",
    "    Returns:\n",
    "    df_txns(pandas dataframe):\n",
    "    \"\"\"\n",
    "    df_checking_txns = fetch_checking_acct_txns(bank_report)\n",
    "    if df_checking_txns.empty is False:\n",
    "        df_txns = df_checking_txns.loc[df_checking_txns['account_number'] == primary_account, :]\n",
    "        df_txns['posted_date'] = pd.to_datetime(df_txns['posted_date'])\n",
    "        df_txns['LoanId'] = loan_id\n",
    "        return df_txns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This function returns the number of income sources and the income cycle of each income source separated by +. For ex, if income sources are 2, the income cycles would be like in_cycle1+in_cycle2, where in_cycle1 is the income cycle of the first income source and similarly the second part for the second income respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_income_sources_and_cycle(json_string, loan_id):\n",
    "    try:\n",
    "        no_sources = json.loads(json_string)['incomeReview']['data']['incomeSources']\n",
    "        cycles = []\n",
    "        for income in range(int(no_sources)):\n",
    "            cycles.append(json.loads(json_string)['incomeReview']['data']['sources'][income]['incomeCycle'])\n",
    "        cycles = '+'.join(cycles)\n",
    "        return [loan_id, no_sources, cycles]\n",
    "    except:\n",
    "        return [0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The below function checks that if payroll category is present in the candidates transaction, it would return all credit transactions that were not of income type. If we dont find any payroll type category, we would just check all the credit transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_income(primary_acct, bank_report, loanid):\n",
    "    test_transact = get_transaction_time_series(primary_acct, bank_report, loanid).sort_values(by = 'posted_date').reset_index(drop = True)\n",
    "    test_transact['LoanId'] = loanid\n",
    "    test_inc = test_transact[(test_transact['amount'] > 0) & ((test_transact['category'] == 'Income') | (test_transact['category'] == 'Payroll') | (test_transact['category'] == 'Paycheck'))][['posted_date', 'amount', 'category', 'type', 'memo']]\n",
    "    list_ = test_inc.to_dict('records')\n",
    "    return list_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The below function transforms the bank statement like the loan ids are of float type, it coverts it into str type and strips off the date part from timeadded feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_statement(df):\n",
    "    \"\"\"Modifies extracted bank statement data\n",
    "    Args:\n",
    "        df (pandas df): Dataframe consisting all the required columns from predicon model database\n",
    "    Returns:\n",
    "        pandas df: Modified dataframe\n",
    "    \"\"\"\n",
    "    df['LoanId'] = df['LoanId'].astype(str).map(lambda x : x.split('.')[0])\n",
    "    df['TimeAdded'] = pd.to_datetime(df['TimeAdded'].map(lambda x : x.date()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The below function converts the transaction statement into 4 buckets based on category of the transactions viz. Payroll, Paycheck, Income or Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diff_inc_cat(k):\n",
    "    txn_proll = txn_pcheq = txn_inc = txn_rest = pd.DataFrame()\n",
    "    if 'Payroll' in pd.DataFrame(income_temp[k]).groupby('category', as_index = False)['posted_date'].count().nlargest(8, 'posted_date').category.values:\n",
    "        txn_proll = pd.DataFrame(income_temp[k])[pd.DataFrame(income_temp[k])['category'] == 'Payroll']\n",
    "    if 'Paycheck' in pd.DataFrame(income_temp[k]).groupby('category', as_index = False)['posted_date'].count().nlargest(8, 'posted_date').category.values:\n",
    "        txn_pcheq = pd.DataFrame(income_temp[k])[pd.DataFrame(income_temp[k])['category'] == 'Paycheck']\n",
    "    if 'Income' in pd.DataFrame(income_temp[k]).groupby('category', as_index = False)['posted_date'].count().nlargest(8, 'posted_date').category.values:\n",
    "        txn_inc = pd.DataFrame(income_temp[k])[pd.DataFrame(income_temp[k])['category'] == 'Income']\n",
    "    txn_rest = pd.DataFrame(income_temp[k])[(pd.DataFrame(income_temp[k])['category'] != 'Income') & (pd.DataFrame(income_temp[k])['category'] != 'Paycheck') & (pd.DataFrame(income_temp[k])['category'] != 'Payroll')]\n",
    "    return txn_proll, txn_pcheq, txn_inc, txn_rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching bankreports and modifying them, then fetching bankapp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bankreports = fetch_required_bank_reports(\"'2020-01-07'\", \"'2020-02-07'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bankapp = fetch_required_bank_app(\"'2020-01-07'\", \"'2020-02-07'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bankreports = modify_statement(df_bankreports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bankreports.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bankapp.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the bankapp data with and bankreports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df_bankapp, df_bankreports, on = 'LoanId', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the multiprocessing function to fetch the name of primary checking accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp.Pool(processes = NCPU) as pool:\n",
    "        result_primary_accts = pool.map(get_primary_account, df['BankReportData'])\n",
    "    \n",
    "df['primary_account'] = result_primary_accts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarly, filtering only those candidates that have atleast 60 transaction days!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp.Pool(processes=NCPU) as pool:\n",
    "        txn_days_count = pool.starmap(get_transaction_days_count, zip(df['primary_account'], df['BankReportData']))\n",
    "\n",
    "df['txn_days_count'] = txn_days_count\n",
    "\n",
    "has_gt_60_days_txns = (df['txn_days_count'] == True)\n",
    "df = df[has_gt_60_days_txns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching the income sources and their cycles respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp.Pool(processes = NCPU) as pool:\n",
    "        source_and_cycle = pool.starmap(get_income_sources_and_cycle, zip(df['json'], df['LoanId']))\n",
    "        \n",
    "df_source_and_cycle = pd.DataFrame(source_and_cycle, columns = ['LoanId', '#sources', 'in_cycles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_source_and_cycle.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the sources and cycle data we got to our previously available dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, df_source_and_cycle, on = 'LoanId', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking a look at how income sources and cycles are distributed accross the loanids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda.countplot_categorical_columns(df, cols = ['#sources', 'in_cycles'], force = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching required data for a particular index from the dataframe to crossverify things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[195, ['BankTransactionId', 'primary_account', 'EmployerName', 'LoanId']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the json file to see the income status of an applicamt. Here, 47 is the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "json.loads(df.loc[195, 'json'])['incomeReview']['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the get_income function to fetch all the required income types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp.Pool(processes = NCPU) as pool:\n",
    "        income_temp = pool.starmap(get_income, zip(df['primary_account'], df['BankReportData'], df['LoanId']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of how our fetched data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "income_temp[317]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loanids which were faulted by manual agents(I think so because they could have a different definition of how an income should behave in the bank statement. may be correct..may be not!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>81672524189</br>\n",
    "<br>20669855168</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The income_temp variable is a list of lists of dictionaries. Converting it to dataframe to analyse it rigorously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(income_temp[195]).set_index('posted_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing the buckets returned by get_diff_inc_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txn_proll, txn_pcheq, txn_inc, txn_rest = get_diff_inc_cat(195)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txn_proll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "txn_inc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "txn_pcheq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "txn_rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Occurences like business services and music!! causing problems(index425)</br>\n",
    "<br>Occurences where income comes from 'income' category with electronic deposits are hard to isolate(index152)</br>\n",
    "<br>Paychecks are always correct</br>\n",
    "<br>Mostly whenever there are payrolls, income is not but vice versa not true!!</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing in which categories do most of the income fall!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_inc = df[df['#sources'] == '1'].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_cat_from_json(json_string):\n",
    "    json_data = json.loads(json_string)['incomeReview']['data']\n",
    "    source_Name = json_data['sources'][0]['sourceName']\n",
    "    cat = []\n",
    "    try:\n",
    "        for i in json_data['incomeTransactions']:\n",
    "            if i['memo'] == source_Name:\n",
    "                cat.append(i['categoryName'])\n",
    "    finally:\n",
    "        return set(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "json.loads(df_analyse.loc[200, 'json'])['incomeReview']['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp.Pool(processes = NCPU) as pool:\n",
    "        cat_temp_1_inc = pool.map(analyse_cat_from_json, df_1_inc['json'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_1_inc = [x[0] for x in cat if len(x) != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predicon_uw_model",
   "language": "python",
   "name": "predicon_uw_model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
